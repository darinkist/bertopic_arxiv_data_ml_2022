{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gU32ZQQiGah_IMbwq_uRIxK5J3vA8c3d",
      "authorship_tag": "ABX9TyPVcbowC92LbEGfknbDxC3K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darinkist/bertopic_arxiv_data_ml_2022/blob/main/BERTopic_ArxivData_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing needed packages\n",
        "!pip install bertopic"
      ],
      "metadata": {
        "id": "xP0SooY8wKH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "1bomJFOwuDHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have downloaded the zip file from\n",
        "# https://www.kaggle.com/Cornell-University/arxiv\n",
        "# Run unzip\n",
        "!unzip archive.zip"
      ],
      "metadata": {
        "id": "Ro1ouESZpKQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "import re"
      ],
      "metadata": {
        "id": "pc0DB0s0ocSs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = pd.read_json('arxiv-metadata-oai-snapshot.json', \n",
        "                      lines=True, \n",
        "                      chunksize = 50000)\n",
        "\n",
        "ml_topics = []\n",
        "for chunk in tqdm(chunks):\n",
        "    chunk_df = chunk[chunk.categories.str.contains('cs.LG|cs.AI|stat.ML', \n",
        "                                                   regex=True)].copy()\n",
        "    \n",
        "    if not chunk_df.empty:\n",
        "        chunk_df['latest_version'] = pd.to_datetime(chunk_df['versions'].apply(lambda x: list(x[-1].values())[1]))\n",
        "        \n",
        "        ml_topics.append(\n",
        "            chunk_df.loc[chunk_df['latest_version'].dt.year == 2022, \n",
        "                         ['title', 'abstract','latest_version']]\n",
        "        )\n",
        "        \n",
        "ml_topics_df = pd.concat(ml_topics)\n",
        "\n",
        "\n",
        "ml_topics_df['doc_raw'] = ml_topics_df['title'] + '. ' + ml_topics_df['abstract']"
      ],
      "metadata": {
        "id": "mbevQC8WocgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ml_topics_df)"
      ],
      "metadata": {
        "id": "VsAFmBcOocjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_topics_df.head(3)"
      ],
      "metadata": {
        "id": "pQpCIDnIoclp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner(text):\n",
        "\n",
        "    # Keep only alphanumerical words plus ,.!?\n",
        "    pattern = re.compile(r\"[A-Za-z\\-.,?!]{3,50}\")\n",
        "    return text.str.findall(pattern).str.join(' ')\n",
        "\n",
        "\n",
        "ml_topics_df['doc_clean'] = ml_topics_df['doc_raw'].str.replace('\\n', ' ')\n",
        "ml_topics_df['doc_clean'] = ml_topics_df['doc_clean'].str.replace('https?://\\S+', '', \n",
        "                                                                  case=False)\n",
        "ml_topics_df['doc_clean'] = cleaner(ml_topics_df['doc_clean'])"
      ],
      "metadata": {
        "id": "cUSUsl3iococ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_it(sent):\n",
        "    empty = []\n",
        "    for word, tag in pos_tag(word_tokenize(sent)):\n",
        "        wntag = tag[0].lower()\n",
        "        wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
        "        if not wntag:\n",
        "            lemma = word\n",
        "            empty.append(lemma)\n",
        "        else:\n",
        "            lemma = lemmatizer.lemmatize(word, wntag)\n",
        "            empty.append(lemma)\n",
        "    return ' '.join(empty)\n",
        "\n",
        "ml_topics_df['doc_lem'] = ml_topics_df['doc_clean'].progress_apply(lambda x: lemmatize_it(x))"
      ],
      "metadata": {
        "id": "x8ReLLG0pesE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the processed data to json\n",
        "ml_topics_df.to_json(\"ml_txt_2022_cleaned_lem.json\")"
      ],
      "metadata": {
        "id": "n7ZekyKmpoy0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ml_txt_2022_cleaned = pd.read_json(\"ml_txt_2022_cleaned_lem.json\")\n",
        "# to_json exports date as unix timestamp\n",
        "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html\n",
        "ml_txt_2022_cleaned[\"latest_version\"] = pd.to_datetime(ml_txt_2022_cleaned[\"latest_version\"], unit=\"ms\")"
      ],
      "metadata": {
        "id": "4KoMMUPxurRy"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "# This might take >1h\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = embedding_model.encode(ml_txt_2022_cleaned['doc_lem'].values, show_progress_bar=True)\n",
        "np.save(\"ml_2022_embeddings\", embeddings)"
      ],
      "metadata": {
        "id": "-6N3ung7v_wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommendation to store the embeddings - so that you do not have to go\n",
        "# through the processing again when trying out different hyperparameters\n",
        "embeddings_saved = np.load(\"ml_2022_embeddings.npy\")"
      ],
      "metadata": {
        "id": "F3RIF6-Yxvs4"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "umap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05)\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=60, min_samples=40,\n",
        "                        gen_min_span_tree=True,\n",
        "                        prediction_data=True)\n",
        "\n",
        "stopwords_list = list(stopwords.words('english')) + ['data', 'model', 'models', 'approach', \n",
        "                                                'approaches','problem','problems', 'training', \n",
        "                                                'methods', 'method', 'algorithm', 'algorithms', \n",
        "                                                'use', 'result', 'results', 'show', 'propose', \n",
        "                                                'provide', 'system', 'accuracy', 'href', 'https', \n",
        "                                                'URL', 'github.com', 'github', 'www.github.com'\n",
        "                                               ]\n",
        "\n",
        "vectorizer_model = CountVectorizer(ngram_range=(1, 2),\n",
        "                                   stop_words=stopwords_list\n",
        "                                  )\n",
        "\n",
        "model = BERTopic(\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    language='english',\n",
        "    calculate_probabilities=False,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "FB_4F07yxoT5"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics, probs = model.fit_transform(ml_txt_2022_cleaned['doc_lem'].values, embeddings_saved)"
      ],
      "metadata": {
        "id": "_jrDuRm0zDRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart(top_n_topics=4, n_words=10)"
      ],
      "metadata": {
        "id": "2yngAuogzIlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.visualize_barchart(top_n_topics=4, n_words=10).write_html(\"ml_topics_22.html\")"
      ],
      "metadata": {
        "id": "QPLypwbGzLtF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}